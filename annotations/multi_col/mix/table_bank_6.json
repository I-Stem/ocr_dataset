{"multicolumn": true, "figures": [["0.711471", "0.284654", "0.397843", "0.128768"]], "tables": [[["0.500686", "0.121800", "0.780196", "0.108165"], ",Models,Word,NaN,NaN,Latex,NaN,NaN,Word+Latex,NaN,NaN\n0,,Precision,Recall,F1,Precision,Recall,F1,Precision,Recall,F1\n1,ResNeXt-101 (Word),0.9496,0.8388,0.8908,0.9902,0.5948,0.7432,0.9594,0.7607,0.8486\n2,ResNeXt-152 (Word),0.953,0.8829,0.9166,0.9808,0.689,0.8094,0.9603,0.8209,0.8851\n3,ResNeXt-101 (Latex),0.8288,0.9395,0.8807,0.9854,0.976,0.9807,0.8744,0.9512,0.9112\n4,ResNeXt-152 (Latex),0.8259,0.9562,0.8863,0.9867,0.9754,0.981,0.872,0.9624,0.9149\n5,ResNeXt-101 (Word+Latex),0.9557,0.8403,0.8943,0.9886,0.9694,0.9789,0.967,0.8817,0.9224\n6,ResNeXt-125 (Word+Latex),0.954,0.8639,0.9067,0.9885,0.9732,0.9808,0.9657,0.8989,0.9311\n"], [["0.292059", "0.249962", "0.415882", "0.068171"], ",Models,Word,Latex,Word+Latex\n0,Image-to-Text (Word),0.7507,0.6733,0.7138\n1,Image-to-Text (Latex),0.4048,0.7653,0.5818\n2,Image-to-Text (Word+Latex),0.7121,0.7647,0.7382\n"], [["0.728922", "0.513483", "0.432745", "0.058324"], ",Length,0-20,21-40,41-60,61-80,>80,All\n0,#Total,32.0,293.0,252.0,145.0,278.0,1000.0\n1,#Exact match,15.0,169.0,102.0,28.0,24.0,338.0\n2,Ratio,0.469,0.577,0.405,0.193,0.086,0.338\n"]], "text": "Table 2: Evaluation results on Word and Latex datasets with ResNeXt-{101,152} as the backbone networks Table 3: Evaluation results (BLEU) for image-to-text models on Word and Latex datasets The evaluation results of table structure recognition are shown in Table 3. We observe that the image-to-text models also perform better on the same domain. The model trained on Word documents performs much better on the Word test set than the Latex test set and vice versa. Similarly, the model accuracy of the Word+Latex model is comparable to other models on Word and Latex domains and better on the mixed- domain dataset. This demonstrates that the mixed-domain model might generalize better in real world applications. 5.4 Analysis For table detection, we sample some incorrect examples from evaluation data for the case study. Figure 7 gives three typi- cal errors of detection results. The first error type is partial- detection, where only part of the tables can be identified and some information is missing. The second error type is un-detection, where some tables in the documents can- not be identified. The third error type is mis-detection, where figures and text blocks in the documents are some- times identified as tables. Taking the ResNeXt-152 model for Word+Latex as an example, the number of un-detected tables is 164. Compared with ground truth tables (2,525), the un-detection rate is 6.5%. Meanwhile, the number of mis- detected tables is 86 compared with the total predicted tables being 2,450. Therefore, the mis-detection rate is 3.5%. Fi- nally, the number of partial-detected tables is 57, leading to a partial-detection rate of 2.3%. This illustrates that there is plenty of room to improve the accuracy of the detection mod- els, especially for un-detection and mis-detection cases. For table structure recognition, we observe that the model accuracy reduces as the length of output becomes larger. Tak- ing the image-to-text model for Word+Latex as an example, the number of exact match between the output and ground truth is shown in Table 4. We can see that the ratio of ex- act match is around 50% for the HTML sequences that are less than 40 tokens. As the number of tokens becomes larger, the ratio reduces dramatically to 8.6%, indicating that it is more difficult to recognize big and complex tables. In gen- eral, the model totally generates the correct output for 338 Figure 7: Table detection examples with (a) partial-detection, (b) un-detection and (c) mis-detection tables. We believe enlarging the training data will further im- prove the current model especially for tables with complex row and column layouts, which will be our next-step effort. Table 4: Number of exact match between the generated HTML tag sequence and ground truth sequence 6 Conclusion To empower the research of table detection and structure recognition for document analysis, we introduce the Table- Bank dataset, a new image-based table analysis dataset built with online Word and Latex documents. We use the Faster R- CNN model and image-to-text model as the baseline to eval- uate the performance of TableBank. In addition, we have also created testing data from Word and Latex documents respec- tively, where the model accuracy in different domains is eval- uated. Experiments show that image-based table detection and recognition with deep learning is a promising research direction. We expect the TableBank dataset will release the power of deep learning in the table analysis task, meanwhile fosters more customized network structures to make substan- tial advances in this task. For future research, we will further enlarge the TableBank from more domains with high quality. Moreover, we plan to build a dataset with multiple labels such as tables, figures, headings, subheadings, text blocks and more. In this way, we ","maths":[]}