{"multicolumn": true, "figures": [["0.511863", "0.482654", "0.544902", "0.239055"]], "tables": [[["0.500686", "0.099000", "0.670784", "0.041054"], ",Number of word images,Number of variable images,NaN\n0,,Variable containing a single character,Variable containing a single character and an index\n1,420.0,308,72\n"], [["0.499314", "0.193759", "0.430000", "0.085442"], ",Methods,Precision,Recall,F-score\n0,Method using orientation of gradient [11],0.8638,0.7681,0.8131\n1,Method using DWT [12],0.9263,0.8345,0.878\n2,Method using the fine-tuning of Alexnet,0.9338,0.8658,0.8985\n3,Method using the fine-tuning of ResNet-50,0.9413,0.8825,0.9109\n4,Method using Alexnet and SVM,0.9813,0.9611,0.9711\n5,Method using ResNet-50 and SVM,0.995,0.9895,0.9923\n"]], "text": "TABLE I: Number of word and variable images in the testing datasets TABLE II: Classification accuracy results (Bold value indicates the highest scores of the methods) pair i Ri Fig. 4: Examples of a word (a); a variable contains a single character (b) and a variable contains an index (c) Fig. 5: Representation of feature of variable and word extracted by Resnet-50 B. Performance evaluation 1) Baseline and performance measures: As a baseline, the existing methods in [11] and [12] are used for the classification of variable and word. Moreover, fine-tuning CNNs is used for evaluating the performance of the classification. Fine-tuning a network is the reuse of a tuned network for a new task of classification. The technique is adopted because the number of variable and word images is not enough to train the CNNs from scratch. By using the technique, the variable and word images are classified by the Softmax classifier that is the last layer of the CNNs. In our work, the Precision (P), Recall (R) and F1 score are used for the performance evaluation. Precision is the proportion of the true positives against all the positive results; Recall is the proportion of the true positives against all the true results and F1 score is the harmonic mean of precision and recall. 2) Performance: The performance comparisons of the pro- posed method and existing ones are shown in the table II. Comparing to traditional methods, the uses of CNNs and SVM in our work show higher accuracy. The accuracy in the classification of variable and textual word much improves. The out-performance comes from the fact that the CNNs allow to extract features of images better than existing methods. Fig. 5 illustrates the features of 340 images of each type of variable and word that are extracted by Resnet-50. The feature distribution is represented by using the dimensional reduction technique that is Principal Component Analysis (PCA) [19]. Actually, 1000 features of variable and word images are extracted by using Resnet-50. The PCA technique allows to represent the features in 2-D space efficiently. The feature representation illustrates the classification possibility of variable and textual word. The method using orientation of gradient in [11] heavily relies on the calculation of skew ","maths":[]}