{"multicolumn": true, "figures": [["0.722745", "0.167247", "0.372549", "0.149371"]], "tables": [[["0.283627", "0.185048", "0.390784", "0.191638"], ",NaN,Per-document average,NaN,NaN,Tables found (total=156),NaN\n0,Participant,Recall,Precision,F1-meas.,Complete,Pure\n1,FineReader,0.9971,0.9729,0.9848,142,148\n2,OmniPage,0.9644,0.9569,0.9606,141,130\n3,Silva,0.9831,0.9292,0.9554,149,137\n4,Nitro,0.9323,0.9397,0.936,124,144\n5,Nurminen,0.9077,0.921,0.9143,114,151\n6,Acrobat,0.8738,0.9365,0.904,110,141\n7,Yildiz,0.853,0.6399,0.7313,100,94\n8,Stoffel,0.6991,0.7536,0.7253,79,66\n9,Liu et al. 2,0.3355,0.8836,0.4864,0,29\n10,Hsu et al.,0.4601,0.3666,0.408,39,95\n11,Fang et al.,0.2697,0.7496,0.3967,28,41\n12,Liu et al. 1,0.2207,0.8885,0.3536,0,25\n"], [["0.284314", "0.358203", "0.321961", "0.074686"], ",NaN,Per-document averages,NaN,NaN\n0,Participant,Recall,Precision,F1-measure\n1,Nurminen,0.9409,0.9512,0.946\n2,Silva,0.6401,0.6144,0.627\n3,Hsu et al.,0.4811,0.5704,0.522\n"], [["0.285000", "0.495077", "0.334706", "0.116952"], ",NaN,Per-document averages,NaN,NaN\n0,Participant,Recall,Precision,F1-measure\n1,FineReader,0.8835,0.871,0.8772\n2,OmniPage,0.838,0.846,0.842\n3,Nurminen,0.8078,0.8693,0.8374\n4,Acrobat,0.7262,0.8159,0.7685\n5,Nitro,0.6793,0.8459,0.7535\n6,Silva,0.7052,0.6874,0.6962\n7,Yildiz,0.5951,0.5752,0.585\n"]], "text": "TABLE II RESULTS FOR THE TABLE LOCATION (LOC) SUB - COMPETITION TABLE III RESULTS FOR THE TABLE STRUCTURE RECOGNITION (STR) SUB - COMPETITION ( BASED ON CORRECT REGION INFORMATION ) TABLE IV TABLE STRUCTURE RECOGNITION RESULTS FOR THE COMPLETE PROCESS (COM) - BASED ON THE SYSTEM\u2019S TABLE LOCATION RESULT Our evaluation metrics were found to be a fair representation of the actual quality of the output from the various systems. The combination of completeness and purity with precision and recall on the character level gives a good overall picture of the region detection quality. Similarly, we have found that using cell adjacency relations to evaluate table structure detection enables us to obtain precision and recall measures which are repeatable and accurately reflect the quality of the result. By calculating the results for each document first, we were able to reduce the bias of \u201cdata-heavy\u201d tables on the overall result. A further improvement for the future would be to evaluate regions by calculating the area (in square points) of region overlap instead of counting characters, after \u201cnormalizing\u201d each region first by shrinking it to the smallest region encompassing all characters within its bounds. This would avoid regions containing overprinted or non-printing characters skewing the result. The structure results for the complete process (see Table IV) should also be treated with some caution. A number of systems Fig. 1. Comparison of results with ruled versus unruled tables for the complete process sub-competition returned large false positive regions, whose table structure consisted of only one cell. In many cases, this huge cell only neighboured one or two other cells, and therefore did not raise the overall false positive count significantly. A further issue with our structure recognition metric is in the comparison of adjacency relations by their textual content. Although our normalization routine stripped or replaced most special characters, there were still some remaining encoding issues when evaluating certain approaches. This is a double- edged sword, as removing all non-alphanumeric characters would make it no longer possible to distinguish between cells that do not contain at least one letter or number, of which there were many in our dataset. In the future, we will therefore consider requiring further information about the cell, such as a bounding box, to enable its unique identification. A CKNOWLEDGMENTS This work has been supported by the EU FP7 Marie Curie Zukunftskolleg Incoming Fellowship Programme, University of Konstanz (grant no. 291784), the ERC grant agreement DIADEM (no. 246858) and by the Oxford Martin School (grant no. LC0910-019). R EFERENCES [1] M. C. G\u00f6bel, T. Hassan, E. Oro, and G. Orsi, \u201cA methodology for evaluating algorithms for table understanding in PDF documents,\u201d in ACM Symposium on Document Engineering, 2012, pp. 45\u201348. [2] E. Oro and M. Ruffolo, \u201cPDF-TREX: An approach for recognizing and extracting tables from PDF documents,\u201d in Proc. of ICDAR, 2009, pp. 906\u2013910. [3] B. Kr\u00fcpl and M. Herzog, \u201cVisually guided bottom-up table detection and segmentation in web documents,\u201d in WWW, 2006, pp. 933\u2013934. [4] A. C. e Silva, \u201cMetrics for evaluating performance in document analysis: application to tables,\u201d IJDAR, vol. 14, no. 1, pp. 101\u2013109, 2011. [5] J. Fang, L. Gao, K. Bai, R. Qiu, X. Tao, and Z. Tang, \u201cA table detection method for multipage PDF documents via visual seperators and tabular structures,\u201d in ICDAR, 2011, pp. 779\u2013783. [6] Y. Liu, K. Bai, P. Mitra, and C. L. Giles, \u201cTableSeer: automatic table metadata extraction and searching in digital libraries,\u201d in JCDL, 2007, pp. 91\u2013100. [7] B. Yildiz, K. Kaiser, and S. Miksch, \u201cpdf2table: A method to extract table information from pdf files,\u201d in IICAI, 2005, pp. 1773\u20131785. [8] A. C. e Silva, \u201cParts that add up to a whole: a framework for the analysis of tables,\u201d Ph.D. dissertation, The University of Edinburgh, 2010. [9] H. Strobelt, D. Oelke, C. Rohrdantz, A. Stoffel, D. A. Keim, and O. Deussen, \u201cDocument cards: A top trumps visualization for docu- ments,\u201d IEEE Trans. Vis. Comput. Graph., vol. 15, no. 6, pp. 1145\u20131152, 2009. [10] A. Stoffel, D. Spretke, H. Kinnemann, and D. A. Keim, \u201cEnhancing document structure analysis using visual analytics,\u201d in SAC, 2010, pp. 8\u201312. 1453 ","maths":[]}